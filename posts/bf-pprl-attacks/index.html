<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>You show me your bits, I show you who you are | Eulenbude</title>
<link rel=icon type=image/png href=https://eulenbu.de/favicon-96x96.png sizes=96x96><link rel=icon type=image/svg+xml href=https://eulenbu.de/favicon.svg><link rel="shortcut icon" href=https://eulenbu.de/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://eulenbu.de/apple-touch-icon.png><meta name=apple-mobile-web-app-title content="Eulenbude"><link rel=manifest href=https://eulenbu.de/site.webmanifest><meta property="og:title" content="You show me your bits, I show you who you are | Eulenbude"><meta property="og:description" content="In my previous post in this series, I gave an overview of how Bloom filters are applied in privacy-preserving record linkage, or PPRL for short. Bloom filters obscure the data that is inserted into them while preserving the similarity between nearly identical data records. They do this by running input data through one or multiple hash functions. The hash digests are then used to determine the indices of bits to set in the Bloom filter. If all of this flew over your head, then I highly recommend you give the previous post a read first. Understanding the basics is key to digging deeper into the potential weaknesses that Bloom filters present in record linkage scenarios. "><meta property="og:url" content="https://eulenbu.de/posts/bf-pprl-attacks/"><link rel=stylesheet href=https://eulenbu.de/css/termeul.css><meta name=twitter:card content="summary"><meta name=twitter:title content="You show me your bits, I show you who you are | Eulenbude"><meta name=twitter:description content="In my previous post in this series, I gave an overview of how Bloom filters are applied in privacy-preserving record linkage, or PPRL for short. Bloom filters obscure the data that is inserted into them while preserving the similarity between nearly identical data records. They do this by running input data through one or multiple hash functions. The hash digests are then used to determine the indices of bits to set in the Bloom filter. If all of this flew over your head, then I highly recommend you give the previous post a read first. Understanding the basics is key to digging deeper into the potential weaknesses that Bloom filters present in record linkage scenarios. "><meta name=keywords content="cybersec,privacy,pprl,security,infosec,programming,linkage,record,data,science"></head><body><div class=bg><div class="bg-layer bg-layer-back"></div><div class="bg-layer bg-layer-front"></div><div class="bg-layer bg-layer-overlay"></div></div><div class=page><header class=page-header><nav class="nav main-nav"><ul><li class=nomarker><h1 class=page-logo><a href=https://eulenbu.de/>Eulenbude</a></h1></li><li><a href=https://eulenbu.de/posts>Posts</a></li><li><a href=https://eulenbu.de/termeul>Termeul</a></li></ul></nav></header><main class=page-main><h1>You show me your bits, I show you who you are</h1><p class=post-date>Published on
<time datetime=$creationDateMachine>2022-12-11</time></p><blockquote><p>This post is part of a series on Bloom filter based privacy-preserving record linkage.
If you&rsquo;re new to this series, then I highly recommend you read the first post as a primer on what you&rsquo;re getting yourself into.</p><ul><li><a href=https://eulenbu.de/posts/bf-pprl-intro/>Find duplicates in your datasets with this one weird data structure</a></li><li><strong><a href=https://eulenbu.de/posts/bf-pprl-attacks/>You show me your bits, I show you who you are</a></strong></li><li><a href=https://eulenbu.de/posts/bf-popcnt/>How to count bits at the speed of light</a></li><li><a href=https://eulenbu.de/posts/bf-ncvr/>Becoming one in a million by giving up your data</a></li></ul></blockquote><p><a href=https://eulenbu.de/posts/bf-pprl-intro/>In my previous post in this series</a>, I gave an overview of how Bloom filters are applied in <strong>privacy-preserving record linkage</strong>, or PPRL for short.
Bloom filters obscure the data that is inserted into them while preserving the similarity between nearly identical data records.
They do this by running input data through one or multiple hash functions.
The hash digests are then used to determine the indices of bits to set in the Bloom filter.
If all of this flew over your head, then I highly recommend you give the previous post a read first.
Understanding the basics is key to digging deeper into the potential weaknesses that Bloom filters present in record linkage scenarios.</p><p>Let&rsquo;s assume the role of a malicious actor.
To link records together, we need so-called <strong>stable identifiers</strong>.
These are attributes of a data record that are highly unlikely to change over a long period of time.
So when we&rsquo;re trying to link personal records, we will usually use highly sensitive personally identifiable information.
As attackers, we&rsquo;re obviously very interested in this type of data and we&rsquo;re going to try our hardest to get our hands on it.</p><p>With Bloom filters, the outlook seems grim.
All we&rsquo;re given for each data record that we&rsquo;re trying to re-identify is the sequence of bits produced by the Bloom filter itself.
This sequence is also known as a <strong>cryptographic long-term key</strong>, or CLK for short.
So to reconstruct the original data from a CLK, we have to somehow find a way to correlate bits in a CLK back to plaintext.
This process is also referred to as <strong>data re-identification or de-anonymization</strong>.</p><p>We know that any set bit in the CLK could&rsquo;ve been caused by any number of inputs.
Furthermore, Bloom filters in record linkage applications will usually make use of some cryptographic hash function like <em>SHA-256</em>.
So as long as one doesn&rsquo;t use hash functions that are dead for good, reconstructing the original data record from a CLK seems infeasible.</p><p>You might have been quick to guess from my overuse of the word &ldquo;seems&rdquo; that it&rsquo;s indeed very much possible to reconstruct data from CLKs.
In this post, I will illustrate some basic attacks, show how to mitigate them, and go deep into one slightly more sophisticated but powerful attack.</p><h2 id=bits-imitate-life>Bits imitate life</h2><p>Most of the attacks that I&rsquo;m about to present have been collected by Vatsalan et al.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> in an effort to provide an overview of present-day PPRL techniques.
I highly recommend having a look at the referenced work as a starting point if you wish to learn more about PPRL beyond what I&rsquo;m telling you about in this series.
It&rsquo;s also good practice in clicking through <em>its</em> references if you ever get stuck.</p><p>Let&rsquo;s assume that we have access to a large collection of CLKs generated from an equally large dataset filled with juicy personal data.
All we know is that these CLKs were generated using Bloom filters.
As mentioned before, Bloom filters in record linkage make use of cryptographic hash functions.
In this context, the challenge of re-identifying data from bit patterns of a CLK is similar to that of guessing a password given a hash digest.</p><p>As is common practice (or as it should be!), when you sign up for a service on the internet, the password you enter is never stored in plaintext.
Instead, it is run through a cryptographic hash function and the hash digest is what&rsquo;s stored in the service database<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.
When a database leak occurs, no cleartext passwords are leaked.
Attackers are stuck with the task of guessing passwords and running them through the same hash function by brute force.</p><p>The main difference between CLKs and password hashes is that passwords are significantly more complex than personal data.
Public data sources of common first and last names, street and city names, phone numbers and e-mail addresses are readily available and can be found very easily in many languages.
Knowing this, a simple &ldquo;brute force&rdquo;-type attack on CLKs can be executed like this:</p><ol><li>Collect personal data from public sources.</li><li>For every personal attribute, generate CLKs for all of its values Bloom filters.</li><li>Check for overlaps between the generated CLKs and the CLKs to re-identify.</li></ol><p>This is an example of a <strong>dictionary attack</strong> on Bloom filters.
The list of CLKs you&rsquo;re trying to identify will usually be generated from more than one attribute.
They are also called <strong>composite CLKs</strong>, and you can think of them as multiple single-attribute CLKs combined into one with a binary OR.
This serves to explain why generating the CLKs for single attributes first is important for this attack to work.</p><p><img src=pprl-dictionary-attack-or.png alt="Diagram demonstrating the generation of a composite CLK from two single-attribute CLKs. All set bits from the single-attribute CLKs are also contained within the composite CLK."></p><p>Say you have a list of known first and last names.
You generate the CLKs for each first and last name.
Then you check them against the list of CLKs that you wish to re-identify.
You find that one of them overlaps with the CLKs for the first name &ldquo;John&rdquo; and last name &ldquo;Doe&rdquo;.
Consequentially, you just found the first and last name that likely generated the bit pattern for the person named &ldquo;John Doe&rdquo;.</p><p>The solution to mitigating a dictionary attack is dead simple.
Instead of equipping a Bloom filter with a plain cryptographic hash function, one should used a <strong>keyed-hash message authentication code</strong> (HMAC) instead.
As long as the key is not known to the attacker, it is not possible to re-identify the personal data that was used to generate a CLK without an exhaustive key search.
However, there is another simple attack that works even with the use of HMACs.</p><p>The distribution of names is not random.
Some names are bound to be more common than others because of their sustained popularity over years and decades.
In Germany, the most common surname you&rsquo;ll find at the time of writing this post is &ldquo;Müller&rdquo;, which is more than twice as common as the third-ranking surname &ldquo;Schneider&rdquo;<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.
So if you were to look at a large group of people, some names (and personal attributes in general) are simply going to be more common than others.</p><p>This carries over into the world of record linkage.
Keep in mind that Bloom filters obscure the data that is inserted into them while preserving their similarity.
However, this also means that the CLKs they generate preserve some statistical properties of the dataset they&rsquo;re generated from.
Assuming that you&rsquo;re given a list of CLKs that is sufficiently large, then there&rsquo;s bound to be bit patterns that are more common than others.
We can perform a <strong>frequency attack</strong> like so:</p><ol><li>Collect frequency tables of personal data from public sources.</li><li>Sort the tables by frequency in descending order.</li><li>Group and count equal (or similar) bit patterns in the CLKs to re-identify.</li><li>Sort the list of CLKs by frequency in descending order and align with the frequency table of known values.</li></ol><p>The attack isn&rsquo;t perfect and, evidently, doesn&rsquo;t work when the list of CLKs you&rsquo;re attacking is small.
But it&rsquo;s still good to be able to make an educated guess about the pieces of data that were used to generate a CLK.
A frequency attack of this kind would just be one step of many to fuse several information sources together to, eventually, be able to perform a successful re-identification attack.</p><p>A possible mitigation is to salt data before inserting it into a Bloom filter.
In password hashing, random characters (making up the &ldquo;salt&rdquo;) are appended to a user&rsquo;s password before hashing to make sure that the resulting hash digests cannot be reversed using a lookup table.
However, this doesn&rsquo;t work the same way in the world of record linkage with Bloom filters.
Let&rsquo;s pretend we add the same salt to text tokens before we insert them into a Bloom filter.
All that&rsquo;s going to do is to scramble the resulting bit patterns.
There will still be more and less frequent bit patterns that align with real-world frequency distributions.</p><p>The solution is to choose a record-specific salt.
This means that in addition to the personal attributes you want to use for matching, e.g. first and last name, gender and birth date, you will need an <strong>extra stable identifier that is sufficiently different from person to person</strong>, and that is likely to be present in the datasets you&rsquo;re trying to link together.
Not only that, but this identifier must be free of errors across data sources.
Choosing a salt like this and appending it to every text token generated from a data record before insertion will, indeed, yield vastly different bit patterns.
However, finding such a salt is not trivial in most cases, since you&rsquo;ll want to use most available stable identifiers for matching, not for salting.</p><p>Another more recently suggested mitigation that doesn&rsquo;t require any additional data is a technique called <strong>XOR folding</strong>, as proposed by Schnell et al.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>
This is achieved by splitting a CLK in two halves, then combining these halves using a binary XOR operation.</p><p><img src=pprl-xor-fold.png alt="Diagram demonstrating XOR folding on a CLK. The CLK is split in two halves. These halves are then combined using an XOR operation, yielding the folded CLK."></p><p>The reason why this makes attacks harder is that inferring frequency information is significantly harder for the folded CLK.
For every set or unset bit in the folded CLK, there are two possible bit combinations that could&rsquo;ve generated it.
So for a folded CLK of length <em>m</em>, there are <em>2<sup>m</sup></em> possible halves of a CLK that have to be considered in the frequency analysis.
This technique does impact record linkage quality negatively, but not drastically according to the current state of research.</p><h2 id=the-key-to-bloom-filters>The key to Bloom filters</h2><p>There&rsquo;s a detail about the dictionary and frequency attack that I left out so far, and without it, you might be wondering how on earth these attacks can even be successful.
To recap, in order to generate a CLK using Bloom filters, there are three parameters that need to be known: the Bloom filter size <em>m</em>, the size of the text tokens that are inserted into the Bloom filter <em>q</em> and the amount of hash values to generate per token <em>k</em>.
How do you go about performing a dictionary attack without the knowledge of these parameters?</p><p>The answer is: you don&rsquo;t need to know.</p><p>The Bloom filter size <em>m</em> directly correlates with the size of the CLK, so very little guesswork is required, if any.
Guessing the token size <em>q</em> is just as trivial, for a simple reason: <strong>the higher the token size, the less error-resistant the linkage</strong>.</p><p>Let&rsquo;s take my first name for example: Maximilian.
With a token size of two, my name splits into nine unique tokens.
By including padding characters for the first and last letter, this sums up to 11 tokens in total.
Now let&rsquo;s pretend someone slipped on the keyboard while typing my name, turning Maximilian into Macimilian.
If we were to tokenize both names, we&rsquo;d find that all except two tokens would match, namely the ones caused by the typo of replacing &ldquo;x&rdquo; with &ldquo;c&rdquo;.
Or, statistically speaking, with 9 out of 11 matched tokens, there&rsquo;d be a 81.8 % overlap.</p><table><thead><tr><th style=text-align:right><strong>q</strong></th><th style=text-align:right><strong>Tokens for Maximilian</strong></th><th style=text-align:left><strong>Tokens for Macimilian</strong></th></tr></thead><tbody><tr><td style=text-align:right>2</td><td style=text-align:right>&mldr;, MA, <strong>AX</strong>, <strong>XI</strong>, IM, &mldr;</td><td style=text-align:left>&mldr;, MA, <strong>AC</strong>, <strong>CI</strong>, IM, &mldr;</td></tr><tr><td style=text-align:right>3</td><td style=text-align:right>&mldr;, _MA, <strong>MAX</strong>, <strong>AXI</strong>, <strong>XIM</strong>, IMI, &mldr;</td><td style=text-align:left>&mldr;, _MA, <strong>MAC</strong>, <strong>ACI</strong>, <strong>CIM</strong>, IMI, &mldr;</td></tr><tr><td style=text-align:right>4</td><td style=text-align:right>&mldr;, __MA, <strong>_MAX</strong>, <strong>MAXI</strong>, <strong>AXIM</strong>, <strong>XIMI</strong>, IMIL, &mldr;</td><td style=text-align:left>&mldr;, __MA, <strong>_MAC</strong>, <strong>MACI</strong>, <strong>ACIM</strong>, <strong>CIMI</strong>, IMIL, &mldr;</td></tr></tbody></table><p>If we were to increase the token size to three, including padding, my first name would split into 12 tokens.
Assuming one typo as before, there&rsquo;d be three mismatched tokens this time, yielding an overlap of merely 75 %.
This percentage keeps dropping with higher token sizes.</p><p>Remember that one of the reasons Bloom filters are so popular in record linkage is that they provide error tolerance for free.
This assumes that the token size is set to a reasonable value though.
In most papers I&rsquo;ve come across, the token size is usually set to two or three because the drop in linkage quality with higher token sizes is simply too drastic to be practical.
Fortunately, for us as attackers, this means that the possible values for <em>q</em> are manageable.</p><p>At this stage, we can make reasonable assumptions about <em>m</em> and <em>q</em>, which only leaves the amount of hash values per token <em>k</em> on the table.
It&rsquo;s possible to simply try different values for <em>k</em>, since <em>k</em> is the only unknown parameter at this stage.
But with a bit of math, one can trim down the options for <em>k</em> even further.
Feel free to skip over the following section if you just want to take my word for it.
It&rsquo;s not required to understand the final attack that I&rsquo;m about to present.
But it never hurts to dig a bit deeper.
(And I&rsquo;m yet to see it written down anywhere so I&rsquo;m glad to be the first if that&rsquo;s the case.)</p><blockquote><p>Let&rsquo;s assume a Bloom filter consisting of <em>m</em> bits.
We can assert that the hash function may select any of the <em>m</em> bits with equal probability.
This means that if we insert an element into the Bloom filter, every bit has a <em>1/m</em> probability of being set, or <em>m<sup>-1</sup></em>.
The opposite event, namely that a bit remains unset, has a probability of <em>1 - m<sup>-1</sup></em> by consequence.</p><p>Now let&rsquo;s assume that we have <em>n</em> elements that we want to insert into the Bloom filter.
For every element, the hash function will generate <em>k</em> hash values.
This means that a total of <em>kn</em> bits will be selected.
So the probability that a bit remains unset even after all these insertions is <em>(1 - m<sup>-1</sup>)<sup>kn</sup></em>.
Let&rsquo;s call this probability <em>p</em>.</p><p>Assuming we have a list of CLKs, we can count the amount of unset bits in each one of them.
Knowing <em>m</em>, we can compute <em>p</em> for every CLK.
Furthermore, we can rearrange the formula to compute <em>p</em> to solve for <em>kn</em> instead.
This yields <em>kn = (log p) / (log (1 - m<sup>-1</sup>))</em>.</p><p><img src=pprl-k-guess.png alt="Formula displaying the probability that a bit is not set in a Bloom filter of size m after n insertions with k hash values each. The formula is then rearranged to kn."></p><p>At the end we obtain <em>kn</em> for every CLK.
This seems useless because we don&rsquo;t know <em>n</em>.
However it&rsquo;s <em>k</em> that we&rsquo;re interested in, and we know <em>k</em> is an integer.
<strong>By consequence, <em>kn</em> must be the multiple of an integer.</strong>
In reality, <em>kn</em> will always be some floating-point number.
However, consider you run this calculation for a list of CLKs and you get results for <em>kn</em> like <em>9.144</em>, <em>12.266</em>, <em>15.427</em>, <em>18.627</em> and <em>21.869</em>.
Stripping the decimal parts, all these numbers are multiples of three, meaning that it&rsquo;s highly likely that <em>k=3</em> in this example.
This is a very simple calculation which will leave you with very few options for <em>k</em>.</p></blockquote><p>Why am I talking about this?
The answer lies in <strong>Kerckhoffs&rsquo;s principle</strong>.
It&rsquo;s the principle that a cryptosystem must be secure even if all details about it, except the key, are leaked.
We&rsquo;re dealing with sensitive data, so keeping it secure should be our primary objective.
So what is the (literal and figurative) key that&rsquo;s supposed to make Bloom filters in record linkage secure?</p><p>After this section, it should be clear that the basic masking parameters <em>m</em>, <em>q</em> and <em>k</em> are not the key.
Tweaking them does not mitigate the risk of re-identification attacks without greatly impacting the quality of record linkage results.
However, we now know two components that we can make use of that directly impact the efficiency of the aforementioned attacks.</p><ul><li>When using an HMAC as the hash function inside a Bloom filter, its <strong>secret key</strong>.</li><li>When using an additional stable identifier to salt tokens before they&rsquo;re inserted into a Bloom filter, the identifier as a <strong>salting key</strong>.</li></ul><p>But if you&rsquo;re still not convinced that basic Bloom filters aren&rsquo;t safe for record linkage, there is one more attack that I&rsquo;d like to dig into.
It&rsquo;s special because not only is it fast to perform, it also doesn&rsquo;t require any knowledge of Bloom filter parameters.</p><h3 id=being-called-peter-müller-is-a-security-risk-in-germany>Being called &ldquo;Peter Müller&rdquo; is a security risk in Germany</h3><p>Christen et al.<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> were the first ones to describe this attack.
Once again, we&rsquo;re exploiting the fact that certain bit patterns are bound to appear more often than others with large datasets.
We mount a frequency attack.
This means we have a list of CLKs generated from a large dataset and a frequency table of personal data.
We group similar CLKs together, count them, and align both lists in descending order.
Furthermore, we will also split the values in our dataset into tokens.</p><p><img src=pprl-crypt-align.png alt="Table showing several CLKs sorted by frequency in descending order. A list of names is sorted in the same way and aligned with the CLKs. The names are split into tokens of size two."></p><p>Keep in mind that in order for a bit to be set in a CLK, at least one text token has to have been hashed into that position.
Our goal with this attack is to have a set of candidate tokens for every bit position.
We will use these sets to limit the amount of possible values that could have caused the bit pattern in a CLK.</p><p>This attack is best explained by running an example.
For the sake of this demonstration, let&rsquo;s assume that we know our CLKs have one single value hashed into them.
Suppose we have a list of names where &ldquo;John&rdquo; ranks first and &ldquo;Joe&rdquo; ranks second.
We also have a list of CLKs ordered by frequency.
In this attack, we look at all bit indices from <em>0</em> to <em>m-1</em> for each CLK.</p><p>Suppose the first bit of the most frequent CLK is set.
Since it ranks the same as &ldquo;John&rdquo;, we add the tokens of &ldquo;John&rdquo; to the set <em>C[0]<sub>pos</sub></em>.
Don&rsquo;t worry about the name of the set for now.
It&rsquo;ll make sense later.
Next, suppose the first bit of the second most frequent CLK is unset.
Since it ranks the same as &ldquo;Joe&rdquo;, we add the tokens of &ldquo;Joe&rdquo; to the set <em>C[0]<sub>neg</sub></em>.</p><p>We repeat this process for the first bit of all CLKs.
At the end of this process, <em>C[0]<sub>pos</sub></em> and <em>C[O]<sub>neg</sub></em> contain tokens that, respectively, may or may not have been hashed into this position.
Finally, we construct the set of candidate tokens for the first bit <em>C[0]</em> by removing the elements in <em>C[0]<sub>neg</sub></em> from <em>C[0]<sub>pos</sub></em>.</p><p><img src=pprl-crypt-token-set.png alt="Diagram showing the construction of the candidate token set for a bit. In four CLKs, two have their first bit set, two don&rsquo;t. The tokens of the similarly ranked names are added to a &ldquo;positive&rdquo; and a &ldquo;negative&rdquo; candidate token set respectively. The candidate token set is then constructed by subtracting the tokens in the &ldquo;negative&rdquo; from the &ldquo;positive&rdquo; candidate token set."></p><p>This is repeated for all bits.
At the end, there&rsquo;ll be a set of candidate tokens for every bit.
Once that&rsquo;s done, we go through our CLKs one by one.
For every set bit of a CLK, we consider its set of candidate tokens.
At the beginning, we assume that all possible values from our frequency table could represent the CLK.
For every value, we check if it has any overlapping tokens with the candidate token set.
If that&rsquo;s not the case, we no longer consider the value to be a candidate for the CLK.</p><p><img src=pprl-crypt-reidentify.png alt="Candidate token sets for every bit position are used to check against the tokens of the most frequent names. For every set bit of a CLK, the list of candidate values are checked against it. If a value doesn&rsquo;t have any overlapping tokens, it is excluded."></p><p>By the end, we should be left with just a couple candidate values for each CLK.
If we&rsquo;re lucky, we&rsquo;re left with a single one.
If not, there&rsquo;s still the chance of having a few candidates to choose from.
Cross-referencing them with candidate values for other CLKs can improve results further.</p><p>This attack serves to show that by making educated guesses about the frequency distribution of CLKs, we can trim down the possible values that could make up a CLK significantly.
And, again, bear in mind that we didn&rsquo;t need to know any Bloom filter properties.
Well, except for the token size, of course, which can be guessed pretty easily.</p><p>The authors of the paper where the attack was first described used the <strong>North Carolina Voter Registry dataset</strong>, which is an open and popular dataset containing about 5 million real-world voter records spanning many years.
It is commonly used for the evaluation of record linkage algorithms.
According to their findings, the attack becomes noticeably less efficient the more attributes are being hashed into a Bloom filter and the more values we&rsquo;re trying to reidentify.
However, there was no mention of how many CLKs were necessary to execute a successful attack.
The authors state that about &ldquo;10&ndash;100&rdquo; CLKs were required, but didn&rsquo;t elaborate any further.</p><p>So I re-implemented the attack and put it to the test.
<a href=https://github.com/JoogsWasTaken/bf-pprl-attack>You can find the code on GitHub here</a>.
I grabbed a table of the 1,000 most popular German first names and their absolute frequencies.
Then I picked the top 10, 25, 50 and 100 names and generated CLKs for them.
The frequencies for these CLKs were randomly drawn based on the frequencies of the names they were generated from, meaning that the distribution of CLKs is as close to the real-world name distribution as possible.
In three different runs, I generated 100k, 10k and 1k total CLKs to see how the amount of CLKs affect the performance of the attack.
The described test setup is designed to have the best possible prerequisites for an attacker to, theoretically, achieve great results.</p><p><img src=pprl-attack-n100k.png alt="Success of the outlined attack depending on the amount of values to reidentify. The reidentification of 10, 25, 50 and 100 values is attempted using a dataset of 100k CLKs. Success of the attack degrades the more values are considered."></p><p>The first run with 100s CLKs seems promising since it aligns with the results presented by the authors.
To explain the graph: every CLK is assigned a set of candidate values by the end of the attack.
There are four different conditions that may show up.</p><ul><li><strong>Exact match</strong>: one candidate value, that being the correct value</li><li><strong>Potential match</strong>: several candidate values, one of them being the correct value</li><li><strong>False match</strong>: several candidate values, none of them being the correct value</li><li><strong>No match:</strong> no candidate values</li></ul><p>As stated by the authors, the attack derails the more values there are to consider.
Looking at the names I chose for my implementation of the attack, the reason becomes quite obvious.</p><p><img src=pprl-attack-top-1k-freq.png alt="Plot of absolute frequencies of the top 1k most common first names on a logarithmic scale."></p><p>There are a handful of names that are orders of magnitude more common than others.
Then there are a large sum of names that are popular, but they blend in with all other popular names.
And then there are the rest.
They&rsquo;re by no means uncommon, but due to the sheer amount of names to consider, there isn&rsquo;t much of a difference between, say, the 682<sup>nd</sup> and 683<sup>rd</sup> ranked name.
And since the attack is based on a regular frequency attack, the assumption that the distribution of CLKs and personal attributes greatly influences the success of the attack, even if it becomes more unreliable the more attribute values we consider.
This doesn&rsquo;t hold for names only, but for many types of data in social sciences according to <strong>Zipf&rsquo;s law</strong>.</p><p>So much for the effect of the amount of attribute values we consider in our attack.
This is nothing new since the authors have already found this out themselves, but I find it interesting to look at data myself to compare these kinds of findings.
However, I sought out to look at something else.
So, how does this attack scale when we have fewer CLKs available to us by an order of magnitude?</p><p><img src=pprl-attack-n10k.png alt="Success of the outlined attack depending on the amount of values to reidentify. The reidentification of 10, 25, 50 and 100 values is attempted using a dataset of 10k CLKs. Success of the attack degrades the more values are considered. Significantly fewer correct guesses are made compared to the run with 100k CLKs."></p><p>Yeah, this doesn&rsquo;t look good.
Even when only looking at the top 10 names, the attack could only correctly re-identify four of them.
And again, the more values we consider, the worse it gets.</p><p><img src=pprl-attack-n1k.png alt="Success of the outlined attack depending on the amount of values to reidentify. The reidentification of 10, 25, 50 and 100 values is attempted using a dataset of 1k CLKs. Success of the attack degrades the more values are considered. Fewer correct guesses are made compared to the run with 10k CLKs."></p><p>This, too, doesn&rsquo;t improve when looking at only 1k CLKs.
With the top 50 names, the attack didn&rsquo;t manage to re-identify a single one of them.
However, this gives us a pretty good idea of the additional prerequisites for this attack to work that weren&rsquo;t covered in their entirety in the original paper.</p><p>First, as shown above, there must be a large amount of CLKs available for the frequency distribution of CLKs to take on the shape of the natural frequencies of first names, last names &mldr; you name it.
By &ldquo;large&rdquo;, I mean somewhere in the neighborhood of at least 100k, looking at the data I used.
Also note that errors in the data that generated these CLKs must be at a minimum, since they&rsquo;d mess with the distribution of CLKs otherwise.</p><p>Second, we have to assume that the frequency tables of CLKs and possible values are of similar length.
Having a corpus of only five unique CLKs but more than 1000 candidate attribute values is going to decrease the quality of results by introducing a lot of false positive guesses.</p><p>Third, the effectiveness of this attack is greatly reduced with composite Bloom filters, as described in the introduction of this post.
The more attributes that are hashed into a Bloom filter, the higher the distribution of bit patterns across CLKs.
Frequency information is a lot harder to extract from a bunch of CLKs that way.
Also, a set bit in a CLK won&rsquo;t tell you which attribute is responsible for setting it.</p><p>But still, keep in mind that in a couple (arguably limited) scenarios, the attack can be highly effective.
The information gained from this attack may just be, as stated previously, one puzzle piece in a large-scale attack.
And if you&rsquo;re determined enough to study this very niche use case of Bloom filters for a potential attack, you might as well walk that extra mile.</p><h2 id=whats-next>What&rsquo;s next?</h2><p>The crux of Bloom filters in privacy-preserving record linkage is that they&rsquo;re used on data that is not random.
Frequency-based attacks can be carried out quite easily and provide significant information gain to an attacker to trim down the possible areas to explore.
However, as discussed, there are several mitigation techniques which make these kinds of attacks a lot harder to carry out by giving an attacker a lot more options to consider.</p><p>In the next instalment of this series, I&rsquo;d like to step away from attacks and Bloom filters a bit.
We&rsquo;ll take a look at an operation that has to be executed a lot in PPRL using Bloom filters, and dive into a bit of computer science.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>See: Vatsalan, Dinusha, et al. &ldquo;Privacy-preserving record linkage for big data: Current approaches and research challenges.&rdquo; <em>Handbook of big data technologies</em> (2017): 851-895.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>I know this shouldn&rsquo;t bear mentioning, but my description of how passwords are stored on the web is <strong>extremely simplified</strong> here. Do not take this as gospel and learn from reputable cybersecurity sources instead.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Taken from Forebears&rsquo; &ldquo;Most Common Last Names In Germany&rdquo; &mdash; <a href=https://forebears.io/germany/surnames>URL</a> / <a href=https://web.archive.org/web/20220922090455/https://forebears.io/germany/surnames>Archive</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>See: Schnell, Rainer, and Christian Borgs. &ldquo;XOR-folding for Bloom filter-based encryptions for privacy-preserving record linkage.&rdquo; <em>German Record Linkage Center, NO. WP-GRLC-2016-03, DECEMBER</em> 22 (2016).&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>See: Christen, Peter, et al. &ldquo;Efficient cryptanalysis of bloom filters for privacy-preserving record linkage.&rdquo; Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, Cham, 2017.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></main><footer class=page-footer><hr><nav class="nav footer-nav"><ul><li class=nomarker><a href=https://creativecommons.org/licenses/by/4.0/>CC BY 4.0</a></li><li><a href=https://eulenbu.de/impressum>Impressum</a></li></ul></nav></footer><div class=owl-container><div class=owl><input type=button class=owl-btn><div class=owl-counter>&#9829; &#215; <span class=owl-counter-value>1,337</span></div></div></div></div><script src=https://eulenbu.de/js/background.js></script><script src=https://eulenbu.de/js/owl.js></script></body></html>